
URL: http://ref.web.cern.ch/ref/CERN/CNL/2001/002/cern-computing
<h>30 Years of Computing at CERN - Part 1

<h>Abstract

<p> This is the first of a three-part series, made out of the original
 (excellent) paper written by Paolo Zanella in 1990. I hope that
 it can serve future Computorians to write "IT at CERN: its birth,
 its life ..." 
 - Miguel Marquina (editor)- 

<p>1. INTRODUCTION

<p>The origins of computing machinery can be traced back to 3000
 years ago when the Chinese introduced a primitive form of abacus,
 or 300 years ago when Schickard, Pascal and Leibniz invented
 the first arithmetic machines. For us at CERN it all started
 30 years ago when one of the first electronic digital computers,
 a huge Ferranti Mercury, was installed. It was the beginning
 of a success story which has changed the way to do physics and,
 as a matter of fact, to do any work at all in the Laboratory.

<p>The story goes on and the changes brought in by the new information
 technologies continue to affect us at an astonishing rate. Those
 who witnessed the beginning of the information era were conscious
 of the potential impact of those mar marvellous machines on
 Physics as well as on Society, but they could hardly imagine
 what was to come. In retrospect, we can say that the 30 years
 which separate us from the days of those first generation machines,
 have been years of struggle, of sweat and tea tears, of doubt
 and of painful reappraisal, but also years of discovery, of
 excitement, of achievement and of pride to the actors in this
 unique adventure.

<p>Of those 30 CERN years, I missed only the first three. I did
 not miss, however, the joys of programming first generation
 machines, although in a different environment. Actually, when
 I joined CERN early in 1962, the only machines around were still
 made out of vacuum tubes. I am, therefore, rather well placed
 to tell my personal account of the what, why and how of the
 evolution of automatic computing at CERN.

<p>I shall not even attempt to be exhaustive, let alone objective.
 I shall tell the story as I saw it, or rather as I lived it.
 Instead of compressing 262800 hours into 1, I shall select and
 report only those events which, in my opinion, are either real
 milestones or make an interesting story, and I shall follow
 them through to their consequences, even if this means breaking
 the strict chronological order. The basic features of the large
 high-performance systems which have played a major role in the
 CERN Computer Center are listed in Appendix 1.

<p>2. THE BIG BANG

<p>It took six days for God to build the Universe and a little over
 two years for the Ferranti engineers to produce our Mercury,
 a large assembly of complex circuitry hidden inside a row of
 austere cabinets, making few concessions ons to people's curiosity.
 At about the same time the PS was built in five years and today
 one can build accelerators 1000 times more powerful, always
 in about five years. The Mercury was 1000 times less powerful
 (16.6 kHz) than a modern personal computer which is now mass-produced
 by robotized assembly lines at the rate of 2 or 3 per minute!

<p>The purchasing contract was signed on the 25th of May, 1956 and
 stipulated that the machine with 1024 40-bit words of fast [
 120 microsec to read/write one word from the accumulator] core
 storeand a slow [8.75 msec latency] drum storehaving a total
 capacity of 16384 words of 40 binary digits, would be shipped
 to CERN in early February 1957 and installation shall be completed
 by 1st May, 1957.... The details of the progress of manufacture
 will be submitted at intervals of one month. The Mercury had
 a clock cycle of 60 microsec. It took 180 microsec to add and
 300 microsec to multiply two long words of 40 bits. It had floating
 point arithmetic. Division, however, had to be programmed. It
 is interesting to note the absence of the word software from
 the contract, the only relevant mention being: any programs
 and sub-routines which Ferranti will prepare will be made available
 to CERN free of charge. Unbundling and licensing were unknown
 in the 50's.

<p>The computer actually arrived during the summer of 1958 and passed
 the acceptance tests in October of that year. Following its
 successful introduction, a thick file of orders revealed the
 urgent need for spare valves and of tape bins (a critical device
 in those days of punched paper tape input/output!).

<p>At the end of 1958, the availability of a programming language
 called Autocode, attracted the first users to the computer centre
 and marked the beginning of the 30 years of computing covered
 by this paper. It is interesting to note that the first Mercury
 Autocode compiler was written in 1956 (by R. A. Brooker), two
 years before the appearance of the first FORTRAN compiler! It
 had many features which appeared later in FORTRAN. Due to memory
 limitations, variable names were restricted to a single alphabetic
 character (5-bit Ferranti code). It is worth noting that in
 the mid 50's the European computer industry was still competitive.
 As far as competence, innovative ideas and successful products
 are concerned, European companies like Ferranti, English Electric
 and  Elliott in England, Telefunken in Germany and Bull in France
 had little to envy their American competitors (e.g. ERA/Remington
 Rand, NCR, IBM). The size and the drive of the American market
 were, however, going to make the difference quite rapidly in
 the 60's.

<p>The reasons behind the decision to acquire a Mercury were technical,
 political and financial. Technically the Mercury was definitely
 one of the most advanced machines around. It was considered
 superior to the Ferranti Pegasus, to the Elliott 404, to the
 English Electric 'Deuce', to the Bull 'Gamma 311' and to drum-type
 machines like the IBM 650. Ferranti was set up to produce a
 dozen units and CERN was going to get serial number 6. The fact
 that Harwell and Saclay had ordered Mercury's was given a certain
 weight. CERN had no experience in electronic digital computers
 and it was better to be part of a club of users. As to the price
 (one million Swiss Francs), it was five times cheaper than equivalent
 American machines like the ERA 1101.

<p>Discussions went on for several months at CERN in 1955 and 1956.
 External experts were involved. Fifteen years after Konrad Zuse's
 relay-based Z3 considered by many as the first general purpose
 computer, ten years after the electronic ENIAC, and six years
 after Maurice Wilkes' EDSAC, people were still debating on the
 virtues of binary versus decimal machines, on the importance
 of floating point arithmetic, on the size of words (the Z3 had
 22-bit and the EDSAC 32-bit words; the ENIAC was a decimal machine
 with 10 digits words. The size of central memory was 64 words
 for the Z3, 2n for the ENIAC and 512 for the EDSAQ, and on the
 best way to input and output data (punched film, 5-7-8 channels
 papertape, or cards). Practical problems had to do with the
 unreliability of the hardware, the organization of the operations
 and the recruitment of experts. Obsolescence was already a problem.

<p>CERN decided that the machine should be purely binary, that 32-bit
 words were too short, and that the speed should be as high as
 possible (in particular multiplication time should be under
 a millisecond). It also concluded that the speed and size of
 memory were very important (several tens of thousands of binary
 digits were a clear necessity), while the availability of floating
 point hardware was considered a very desirable, although not
 quite indispensable, feature of a scientific computer. But what
 really makes it worthwhile to have a machine, is the enthusiasm
 for carrying out the most difficult computations, as an American
 physicist put it in a letter describing his experience with
 a digital computer or 'the courage to go ahead and solve problems
 which would have seemed too difficult to do otherwise.'

<p>The computers of that time had colorful names like ILLIAC at
 the University of Illinois, AVIDAC at Argonne, MANIAC, UNIVAC,
 etc... Most of them were prototypes. Everything was very much
 experimental. The operations arrangements at the ILLIAC were
 described as follows: 'The user deposits his punched paper tape
 in a box with instructions or the operator. The operator takes
 these tapes, and inserts them in the machine in turn. If the
 code is correct, the machine delivers the answers and these
 may be picked up by the user the next morning. If there are
 errors in the code, the operator carries out the test routines
 requested and the results of these tests are deposited so that
 these may be reviewed by the user the next morning. Thus, the
 whole operation of the machine becomes a fairly automatic affair'.
 This was more or less the style adopted for our operations thirty
 years ago.

<p>One of the first applications at CERN was the analysis of the
 papertape produced by the Instruments for the Evaluation of
 Photographs (IEPs), used to scan and measure bubble chamber
 film. The first reports convey a certain deception due to the
 slow tape read/write speed. Since everything had to go through
 the accumulator, the CPU was blocked during I/0. It was immediately
 clear that there was a big mismatch between the power of the
 computing engine and its input/output capability.

<p>After some struggling with faulty tubes, tape bins, machine instructions
 and Autocode, people with lots of data discovered the existence
 of an IBM 704 in Paris, which offered significant advantages
 such as magnetic tape units, card readers, line printers and
 FORTRAN! FORTRAN II allowed 6-characters variable names and,
 most important, it simplified the exchange of programs with
 Berkeley and Brookhaven. The 1959 CERN Annual Report indicated
 already that as the needs increase, it will be necessary to
 envisage the replacement of the Mercury by a more powerful system.
 It was also quickly realized that these so-called electronic
 brains required quite a lot of human effort to be effectively
 exploited. Hence the proposal to double in 1960 the computer
 center staff (from 10 to 20).

<p>So, by the end of the 50's, the fundamental forces, sociological
 and technological, characteristic of every computer service,
 had been discovered, including the illusion that upgrading the
 resources would solve all the problems and achieve the ultimate
 goal, i.e. make the users happy!

<p>3. THE LESSONS OF THE EARLY 60'S

<p>The next big news was the arrival of the IBM 709, an improved
 version of the 704, in January 1961. It was still a clumsy vacuum
 tube machine but it featured FORTRAN and all those fancy peripherals
 apt to improve the quality of life.

<p>The word length was 36 bits, the characters became 6-bit BCD,
 and the core memory size jumped to 32K. The CPU was 4-5 times
 faster than that of the Mercury. However, to compile a typical
 FORTRAN program could take several minutes! Tape bins made way
 for card trays. Magnetic tape units read and wrote at 75 ips
 on 7 tracks and the density was 200 bpi. Peripherals were attached
 via their controllers to data channels. It was a significant
 advance in that it allowed as many as six peripheral devices
 to access core memory buffers while the CPU performed other
 work. Another important device which came with the 709 was the
 so-called Direct Data Connection, allowing for direct transmission
 of data from external equipment to memory via a channel. The
 speed was not ridiculous: in principle up to 1 Megabit/sec.
 The 709 was also equipped with one of the first interrupt systems.

<p>The bad news was still the poor reliability, although the progress
 was already quite substantial. Unscheduled maintenance represented
 11% of the total time. Scheduled maintenance took away a time
 slice of similar size. So the down-time of the 709 compared
 not too unfavorably with the up time of the very first electronic
 computers.... The on-line card reader and the printer did, however,
 slow down the operations considerably. After one year of experience
 CERN added a small IBM 1401, in order to speed up the input/output,
 the job sequencing and the operations. The concept of SPOOLing
 (Simultaneous Peripheral Operation On-Line) with its 1/0 files
 (virtual reader/printer) has its origins in those days. Programming
 for the 709 was considered a difficult activity to be left to
 the specialists who could understand and keep up-to-date with
 the new techniques and the operating conventions. The machine
 was an expensive resource which had to be used efficiently.
 To give an idea, the list price was in the region of ten millions
 francs (1960 Swiss francs!). In those days a magnetic tape cost
 60 $ (some 260 SF!). It was at that time that the first inescapable
 committees appeared, e.g. Computer Scheduling Committee, Computer
 Users Advisory Committee and the Data Handling Policy Group.

<p>The Mercury had still its faithful users but suffered from the
 chronic lack of modern, fast peripherals. In 1962, as part of
 a lifting operation, it was enhanced by the connection of an
 Ampex tape unit, compatible with IBM specifications and operating
 at 3333 characters per second (over 3 times the speed of the
 fastest paper tape reader and some 20 times faster than a tape
 punch). Also, two papertape-to-card converters were installed
 to ease the transfer of data from the Mercury to the 709. By
 the end of 1962 it was possible to read the paper tape from
 IEPs into the Mercury, give it a first processing pass, write
 the results on magnetic tape and input it onto the IBM 709 for
 further analysis. The first application packages appeared at
 that time, e.g. THRESH and GRIND used for the geometrical reconstruction
 and kinematic analysis of bubble chamber events. It is amusing
 to note that, in spite of the growing workload and the frantic
 development of codes, the machines were normally switched off
 at weekends. But the practice of 24 hours/day, 7-days/week service
 was around the corner. It is also in interesting to realize
 that things like the connection of the Ampex tape unit to the
 Mercury were entirely designed and implemented on site.

<p>The next problem was how to use all those Autocode programs on
 the 709. One just wrote an Autocode compiler for the 709. The
 difficulties of developing software were soon to be learned.
 The first Conference recognizing the existence of a software
 crisis was held in Munich in 1968. Why is software always late
 and unreliable? People working today with modern CASE (Computer
 Assisted Software Engineering) or OOP (Object Oriented Programming)
 tools are still trying to solve the problem. But the answer
 in those days was: better programming languages.

<p>CERN FORTRAN was defined to ensure compatibility with other laboratories
 and facilitate portability of codes. It was felt, however, that
 FORTRAN was used mainly for historical reasons and new, more
 powerful languages would be needed to fully exploit the potential
 of the electronic computer. As we all know, CERN was not affected
 by, or it missed completely, the language explosion which started
 in the early 60's. ALGOL, Lisp, PL/I, PASCAL, Modula II, ADA,
 C, PROLOG, etc... did not raise above the level of minority
 cultures. FORTRAN evolved through its versions II, IV, 66, 77,
 8X, and it still dominates the CERN programming landscape.

<p>It took some time to saturate the 709, but it was already clear
 that young physicists were becoming addicted. It was at that
 time that the first embryonic Operating System appeared under
 the name of FORTRAN Monitor System. Many other important events
 occurred in the early 60's, such as the connection of computers
 on-line to film measuring devices including the very fast automatic
 flying spot digitizers (HPD, Luciole, etc..) forerunners of
 the modern image digitizers and the first attempts to connect
 computers directly to experimental equipment (on-line experiments).
 The IBM 709 was operated on-line to an HPD to measure both bubble
 and spark chamber films. In September 1963 the 709 was replaced
 by a  7090, a transistorized version of the same machine, about
 four times more powerful.

<p>It was at that time that the investments and the efforts started
 to pay off. Over 300 000 frames of spark chamber film were automatically
 scanned and measured in record time using an HPD Flying Spot
 Digitizer on-line to the 7090. At about the same time computers
 were connected on-line to experiments to monitor the equipment
 and to collect digital data from the first filmless detectors
 (e.g. sonic spark chambers) onto magnetic tape. The first successful
 demonstrations with fully auto automatic digital pattern recognition
 showed that computers could be programmed to replace slow human
 operators in a variety of tasks. Stories about computers doing
 things faster, better and more reliably than human beings got
 around producing the usual mixture re of emotional reactions.
 In 1970 the European Physics Society held a Conference at CERN
 on the 'Impact of Computers on Physics' and I remember the reassuring
 statement of an eminent physicist that 'so far computers have
 not significantly contributed to any discovery'. It was going
 to take another decade to see the HEP community wholeheartedly
 accepting the computer as a critical component of their research
 and admitting it to their current technological foundation trilogy:
  accelerators, detectors and computers.

<p>1970 was also the year when the 'CERN Computing and Data Handling
 School' was launched to educate young physicists and stimulate
 the sharing of computing experience between high-energy physicists
 and computer scientists. It turned out to be an excellent idea.
 The School is still alive and well, fulfilling a clear need.
 As to the cross-fertilization across the physics/computer science
 boundary, it has developed into an ideal partnership.

<p>Actually the two disciplines have influenced each other from
 the very beginning. It was the physicist Bruno Rossi who built
 the first logic circuits which then were developed into computer
 hardware, and physicists have always been among the most demanding
 consumers of computer cycles. Enrico Fermi, when asked in the
 early fifties which research project would he recommend to the
 young Italian physicists, told them to design and build a computer.
 Indeed, physics research could not have become what it is without
 the computer, and conversely, the development of the computer
 has been deeply influenced by the needs and vision of basic
 research.

<p>The study of the fundamental properties of elementary matter
 involves the frontiers of human knowledge and pushes the technology
 to the limit of what is possible. Computer scientists have been
 playing with models and formalisms, architectures and languages,
 inventing tools and methodologies of a rather theoretical nature
 and they have been sometimes accused of developing general solutions
 in search of specific problems. When a dense problem space meets
 a rich solution space some good news can be expected... Evidence
 of synergistic effects has been accumulating ever since the
 beginning of the information era. High-energy physics and information
 technology are among those disciplines which, in the second
 half of our century, have shown the most impressive advances.

<p>Appendix 1 (part 1)

<p>All the major computers having served in the CERN Computer Centre
 in the period 1958-1988 are listed in chronological order, together
 with some configuration details and their characteristic features.

<p>FERRANTI 'Mercury' [1958-1965]

<p>First generation vacuum tube machine (60 microsec clock cycle,
 2 cycles to load or store, 3 cycles to add and 5 cycles to multiply
 40 bit longwords, no hardware division) with magnetic core storage
 (1024 40-bit words, 120 microsec access time). Mercury's processor
 had floating point arithmetic and a B-Register (index register).
 Magnetic drum auxiliary storage (16 Kwords of 40 bits, 8.75
 msec average latency, 64 longwords transferred per revolution).
 Paper tape I/0. Two Ampex magtape units added in 1962. Autocode
 compiler. At the end of its career it was connected on-line
 to an experiment (Missing Mass Spectrometer). In 1966 the Mercury
 was shipped to Poland as a gift to the Academy of Mining and
 Metallurgy at Cracow.

<p>IBM 709 [1961-1963]

<p>Vacuum tube machine (12 microsec clock cycle, 2 cycles to add
 and 15 on average to multiply 36 bit integers, hardwired division
 and floating point arithmetic, index registers) with core storage
 (32 Kwords of 36 bits, 24 microsec access time). Card reader
 (250 cpm) and card punch (100 cpm). Line printer. Magtape units
 (7 tracks, 75 ips, 200 bpi). Introduction of the Data Channel.
 FORTRAN compiler. FORTRAN Monitor System.

<p>IBM 7090 [1963-1965]

<p>Transistorized second-generation machine ( 2.18 microsec clock
 cycle) with core storage (32 Kwords of 36 bits, 4.36 microsec
 access time). Card 1/0, Tape units (7 tracks, 112.5 ips, 200/556
 bpi). Eight Data Channels. Interrupt System. FORTRAN compiler.
 Basic Monitor Operating System (IBSYS). Connected on-line to
 Flying Spot Digitizers (HPD and Luciole) to measure bubble and
 spark chamber films.

<p>CDC 6600 [1965-1975]

<p>Serial Number 3 (pre-production series machine). Transistor machine
 designed by Seymour Cray and very compact for its time. CPU
 clock cycle 100 nsec. Core memory: 128 Kwords of 60 bits. Memory
 access 1 microsec, but independent memory banks allowed for
 up to one access per clock cycle. Instruction prefetch. Ten
 overlapping functional units. Ten autonomous peripheral processor
 units (PPU's) each with 4K of 12-bit words core memory. Huge
 disks over one meter in diameter holding 500 million bits. Tape
 Units (half inch tape, 7 tracks, 200, 556 and 800 bpi, and one
 inch tape, 14 tracks, 800 bpi). High-speed card reader (1200
 cpm).

<p>First multi-programmed machine in the Computer Centre. However,
 SIPROS multiprogramming operating system was abandoned by Control
 Data. Basic SIPROS operating system had to be made at CERN.
 Then Chippewa OS (COS) was installed. It evolved to SCOPE which
 was eventually used after adapting it to CERN needs. This resulted
 in a non-trivial amount of changes, thus deserving the renaming
 to CERN SCOPE. The 6600 was connected to various FSD systems
 and to two on-line computers, the SDS920 and the IBM 1800, via
 CERN-made data links. In terms of processing capacity the 6600
 was about three quarters of a CERN unit or ten times the 7090.

<p>The change-over from the IBM 7090 was planned to take three months
 starting in January 1965. Major engineering overhauls had to
 be done instead during the first few years and ended up in a
 two-months shut-down in 1968 in order to modify the 6600 to
 incorporate logic and packaging improvements which had been
 introduced in the production machines. During this long period
 of struggling with hardware instabilities and software development
 and changes, computing work was done partly by sending jobs
 to outside computers and partly by processing data on a CDC
 3400, and later on a 3800, temporarily made available at CERN
 by Control Data.

<p>CDC 3800 [1966-1968]

<p>The 3800 was a member of the 3000 series CDC family of computers,
 incompatible with the 6000 series machines. More conventional
 than the 6600, the 3800 had a 48-bit architecture. The core
 memory (64 Kwords) was replaced by a faster one (800 nsec) during
 its staying at CERN. This machine was eventually acquired by
 the State of Geneva and installed at the local University. At
 CERN it was replaced by a CDC 6400. It is worth noting that
 CERN acquired other machines of the 3000 s series, e.g. a 3100
 for the FOCUS project offering semi-interactive facilities and
 quick sampling of experimental data at the central computers,
 and a 3200 for interactive graphics applications.



